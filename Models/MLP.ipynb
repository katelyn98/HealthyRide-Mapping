{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "martial-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "psychological-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 layer MLP with one hidden layer\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    #define network\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) # input layer -> hidden layer\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    #define forward function\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "based-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, dataloader, func_loss, opt, num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, batched_data in enumerate(dataloader): \n",
    "            inputs, targets = batched_data\n",
    "#             print(\"inputs: \" + str(inputs.shape))\n",
    "#             print(\"target: \" + str(targets.shape))\n",
    "            \n",
    "            output_prob = model(inputs) #pass images through model to get probability for each class (1 x 10 dim)\n",
    "            loss = func_loss(output_prob.squeeze(), targets)\n",
    "            \n",
    "            #print(output_prob, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward() #performing back prop\n",
    "            opt.step() #updating the weights\n",
    "            opt.zero_grad() #reset gradients\n",
    "            \n",
    "        if epoch % 1 == 0:\n",
    "            print('Epoch: ' + str(epoch) + ' Final loss is: ' + str(loss.item()))\n",
    "            \n",
    "    print('Epoch: ' + str(epoch) + ' Final loss is: ' + str(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "sought-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/PGH/DemandPrediction/ml_df.csv')\n",
    "df = df.drop(columns=['Unnamed: 0', 'median_hh_income','mean_hh_income','in_labor_force', 'civ_labor_force', \n",
    "                      'employed','unemployed', 'perc_employed', 'perc_unemployed','own_alone', \n",
    "                      'carpool', 'public_transit', 'walked', 'other', 'wfh','mean_travel_time_to_work', \n",
    "                      'perc_public', 'perc_alone', 'perc_walk','perc_other', '2018_outflow', '2019_outflow', '2020_outflow'])\n",
    "\n",
    "for row in range(len(df)):\n",
    "    for col in df.columns:\n",
    "        if df.loc[row][col] == '-' or pd.isna(df.loc[row][col]):\n",
    "            df.at[row,col] = 0\n",
    "            \n",
    "\n",
    "X = torch.from_numpy(df.iloc[:, 0:3].to_numpy()).type(torch.float)\n",
    "y = torch.from_numpy(df.iloc[:, 3].to_numpy()).type(torch.float)\n",
    "\n",
    "# mean = torch.mean(X, axis=0)\n",
    "# stdev = torch.std(X, axis=0)\n",
    "# X = (X - mean)/stdev\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "train_ds, test_ds = random_split(dataset, [500, 228])\n",
    "batch_size=1\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "hungry-rider",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = len(df.iloc[:,0:3].columns)\n",
    "output_size = 1\n",
    "input_size,output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "following-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader, func_loss):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        right, running_total = 0, 0\n",
    "\n",
    "        for i, batched_data in enumerate(dataloader):\n",
    "\n",
    "            #moving tensors to the GPU\n",
    "            inputs, targets = batched_data #grab the batch of images as tensors and grab the batch of labels as tensors\n",
    "\n",
    "            output_probs = model(inputs) #pass images through model to get probability for each class (1 x 10 dim)\n",
    "\n",
    "            loss = func_loss(output_probs.squeeze(), targets)\n",
    "            print(\"loss: \" + str(loss))\n",
    "            total_loss += loss\n",
    " \n",
    "        print('Total Loss L1: ' + str(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-industry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "nominated-hawaiian",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katelyncmorrison/opt/anaconda3/envs/CURF/lib/python3.7/site-packages/torch/nn/modules/loss.py:88: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Final loss is: 75.36445617675781\n",
      "Epoch: 1 Final loss is: 39.07301330566406\n",
      "Epoch: 2 Final loss is: 93.38905334472656\n",
      "Epoch: 3 Final loss is: 90.29779052734375\n",
      "Epoch: 4 Final loss is: 75.46408081054688\n",
      "Epoch: 5 Final loss is: 166.05946350097656\n",
      "Epoch: 6 Final loss is: 132.12452697753906\n",
      "Epoch: 7 Final loss is: 132.12359619140625\n",
      "Epoch: 8 Final loss is: 98.99897766113281\n",
      "Epoch: 9 Final loss is: 66.23530578613281\n",
      "Epoch: 10 Final loss is: 55.36125183105469\n",
      "Epoch: 11 Final loss is: 294.28607177734375\n",
      "Epoch: 12 Final loss is: 4.2451019287109375\n",
      "Epoch: 13 Final loss is: 221.8878173828125\n",
      "Epoch: 14 Final loss is: 9.013427734375\n",
      "Epoch: 15 Final loss is: 190.3286590576172\n",
      "Epoch: 16 Final loss is: 88.96755981445312\n",
      "Epoch: 17 Final loss is: 2068.10791015625\n",
      "Epoch: 18 Final loss is: 38.760711669921875\n",
      "Epoch: 19 Final loss is: 75.56967163085938\n",
      "Epoch: 20 Final loss is: 0.20469045639038086\n",
      "Epoch: 21 Final loss is: 4.0063323974609375\n",
      "Epoch: 22 Final loss is: 168.40122985839844\n",
      "Epoch: 23 Final loss is: 341.0020446777344\n",
      "Epoch: 24 Final loss is: 59.86244201660156\n",
      "Epoch: 25 Final loss is: 26.159912109375\n",
      "Epoch: 26 Final loss is: 1.3712830543518066\n",
      "Epoch: 27 Final loss is: 146.0033721923828\n",
      "Epoch: 28 Final loss is: 56.79119873046875\n",
      "Epoch: 29 Final loss is: 38.390777587890625\n",
      "Epoch: 30 Final loss is: 0.9119324684143066\n",
      "Epoch: 31 Final loss is: 93.69903564453125\n",
      "Epoch: 32 Final loss is: 92.603271484375\n",
      "Epoch: 33 Final loss is: 82.15000915527344\n",
      "Epoch: 34 Final loss is: 154.61459350585938\n",
      "Epoch: 35 Final loss is: 0.4456806182861328\n",
      "Epoch: 36 Final loss is: 148.3829345703125\n",
      "Epoch: 37 Final loss is: 0.5221405029296875\n",
      "Epoch: 38 Final loss is: 28.209335327148438\n",
      "Epoch: 39 Final loss is: 73.10240173339844\n",
      "Epoch: 40 Final loss is: 137.25572204589844\n",
      "Epoch: 41 Final loss is: 23.776596069335938\n",
      "Epoch: 42 Final loss is: 48.446258544921875\n",
      "Epoch: 43 Final loss is: 1802.532958984375\n",
      "Epoch: 44 Final loss is: 53.26875305175781\n",
      "Epoch: 45 Final loss is: 29.175704956054688\n",
      "Epoch: 46 Final loss is: 75.41313171386719\n",
      "Epoch: 47 Final loss is: 22.089492797851562\n",
      "Epoch: 48 Final loss is: 155.87393188476562\n",
      "Epoch: 49 Final loss is: 202.41781616210938\n",
      "Epoch: 50 Final loss is: 4.8239898681640625\n",
      "Epoch: 51 Final loss is: 143.1833953857422\n",
      "Epoch: 52 Final loss is: 61.68656921386719\n",
      "Epoch: 53 Final loss is: 44.57328796386719\n",
      "Epoch: 54 Final loss is: 22.788619995117188\n",
      "Epoch: 55 Final loss is: 107.07356262207031\n",
      "Epoch: 56 Final loss is: 44.68989562988281\n",
      "Epoch: 57 Final loss is: 151.7646026611328\n",
      "Epoch: 58 Final loss is: 21.813751220703125\n",
      "Epoch: 59 Final loss is: 1909.56103515625\n",
      "Epoch: 60 Final loss is: 162.24180603027344\n",
      "Epoch: 61 Final loss is: 1765.409423828125\n",
      "Epoch: 62 Final loss is: 131.4596405029297\n",
      "Epoch: 63 Final loss is: 870.3733520507812\n",
      "Epoch: 64 Final loss is: 115.41387939453125\n",
      "Epoch: 65 Final loss is: 25.344009399414062\n",
      "Epoch: 66 Final loss is: 176.90679931640625\n",
      "Epoch: 67 Final loss is: 0.26493120193481445\n",
      "Epoch: 68 Final loss is: 188.5640106201172\n",
      "Epoch: 69 Final loss is: 1.385176181793213\n",
      "Epoch: 70 Final loss is: 129.3131561279297\n",
      "Epoch: 71 Final loss is: 108.54859924316406\n",
      "Epoch: 72 Final loss is: 165.96316528320312\n",
      "Epoch: 73 Final loss is: 12.888717651367188\n",
      "Epoch: 74 Final loss is: 0.05918693542480469\n",
      "Epoch: 75 Final loss is: 79.78129577636719\n",
      "Epoch: 76 Final loss is: 24.2889404296875\n",
      "Epoch: 77 Final loss is: 63.116485595703125\n",
      "Epoch: 78 Final loss is: 65.36117553710938\n",
      "Epoch: 79 Final loss is: 268.79827880859375\n",
      "Epoch: 80 Final loss is: 8.070953369140625\n",
      "Epoch: 81 Final loss is: 1.7751469612121582\n",
      "Epoch: 82 Final loss is: 55.629119873046875\n",
      "Epoch: 83 Final loss is: 136.46421813964844\n",
      "Epoch: 84 Final loss is: 43.33073425292969\n",
      "Epoch: 85 Final loss is: 405.85064697265625\n",
      "Epoch: 86 Final loss is: 74.16278076171875\n",
      "Epoch: 87 Final loss is: 138.17681884765625\n",
      "Epoch: 88 Final loss is: 414.06817626953125\n",
      "Epoch: 89 Final loss is: 117.80461120605469\n",
      "Epoch: 90 Final loss is: 757.5567626953125\n",
      "Epoch: 91 Final loss is: 34.27171325683594\n",
      "Epoch: 92 Final loss is: 3.305375814437866\n",
      "Epoch: 93 Final loss is: 133.00721740722656\n",
      "Epoch: 94 Final loss is: 363.23028564453125\n",
      "Epoch: 95 Final loss is: 95.68838500976562\n",
      "Epoch: 96 Final loss is: 6.0043487548828125\n",
      "Epoch: 97 Final loss is: 22.473373413085938\n",
      "Epoch: 98 Final loss is: 56.5465087890625\n",
      "Epoch: 99 Final loss is: 97.51361083984375\n",
      "Epoch: 100 Final loss is: 10.456512451171875\n",
      "Epoch: 101 Final loss is: 100.18449401855469\n",
      "Epoch: 102 Final loss is: 89.70468139648438\n",
      "Epoch: 103 Final loss is: 95.82630920410156\n",
      "Epoch: 104 Final loss is: 73.56304931640625\n",
      "Epoch: 105 Final loss is: 131.61741638183594\n",
      "Epoch: 106 Final loss is: 1.0396366119384766\n",
      "Epoch: 107 Final loss is: 0.30521488189697266\n",
      "Epoch: 108 Final loss is: 64.78224182128906\n",
      "Epoch: 109 Final loss is: 165.59295654296875\n",
      "Epoch: 110 Final loss is: 1079.5462646484375\n",
      "Epoch: 111 Final loss is: 38.91496276855469\n",
      "Epoch: 112 Final loss is: 311.15277099609375\n",
      "Epoch: 113 Final loss is: 176.6833038330078\n",
      "Epoch: 114 Final loss is: 47.15586853027344\n",
      "Epoch: 115 Final loss is: 166.17227172851562\n",
      "Epoch: 116 Final loss is: 84.9827880859375\n",
      "Epoch: 117 Final loss is: 36.281524658203125\n",
      "Epoch: 118 Final loss is: 58.56892395019531\n",
      "Epoch: 119 Final loss is: 191.7931365966797\n",
      "Epoch: 120 Final loss is: 123.52676391601562\n",
      "Epoch: 121 Final loss is: 362.0584716796875\n",
      "Epoch: 122 Final loss is: 41.589141845703125\n",
      "Epoch: 123 Final loss is: 0.6066265106201172\n",
      "Epoch: 124 Final loss is: 137.3021697998047\n",
      "Epoch: 125 Final loss is: 166.48512268066406\n",
      "Epoch: 126 Final loss is: 216.63973999023438\n",
      "Epoch: 127 Final loss is: 67.29475402832031\n",
      "Epoch: 128 Final loss is: 150.0489501953125\n",
      "Epoch: 129 Final loss is: 81.65467834472656\n",
      "Epoch: 130 Final loss is: 182.6079864501953\n",
      "Epoch: 131 Final loss is: 19.889999389648438\n",
      "Epoch: 132 Final loss is: 64.1494140625\n",
      "Epoch: 133 Final loss is: 77.75325012207031\n",
      "Epoch: 134 Final loss is: 145.67135620117188\n",
      "Epoch: 135 Final loss is: 954.628173828125\n",
      "Epoch: 136 Final loss is: 186.20285034179688\n",
      "Epoch: 137 Final loss is: 370.49249267578125\n",
      "Epoch: 138 Final loss is: 1574.13623046875\n",
      "Epoch: 139 Final loss is: 103.81849670410156\n",
      "Epoch: 140 Final loss is: 86.092529296875\n",
      "Epoch: 141 Final loss is: 18.053192138671875\n",
      "Epoch: 142 Final loss is: 163.24839782714844\n",
      "Epoch: 143 Final loss is: 93.23039245605469\n",
      "Epoch: 144 Final loss is: 1.1909918785095215\n",
      "Epoch: 145 Final loss is: 286.6290283203125\n",
      "Epoch: 146 Final loss is: 12.544624328613281\n",
      "Epoch: 147 Final loss is: 24.440780639648438\n",
      "Epoch: 148 Final loss is: 105.92935180664062\n",
      "Epoch: 149 Final loss is: 39.32513427734375\n",
      "Epoch: 150 Final loss is: 42.79649353027344\n",
      "Epoch: 151 Final loss is: 69.80116271972656\n",
      "Epoch: 152 Final loss is: 2.727023124694824\n",
      "Epoch: 153 Final loss is: 81.3182373046875\n",
      "Epoch: 154 Final loss is: 13.132705688476562\n",
      "Epoch: 155 Final loss is: 349.36871337890625\n",
      "Epoch: 156 Final loss is: 28.5364990234375\n",
      "Epoch: 157 Final loss is: 16.333908081054688\n",
      "Epoch: 158 Final loss is: 98.58479309082031\n",
      "Epoch: 159 Final loss is: 0.14255332946777344\n",
      "Epoch: 160 Final loss is: 0.550879955291748\n",
      "Epoch: 161 Final loss is: 1.4337067604064941\n",
      "Epoch: 162 Final loss is: 112.45262145996094\n",
      "Epoch: 163 Final loss is: 81.87252807617188\n",
      "Epoch: 164 Final loss is: 66.57377624511719\n",
      "Epoch: 165 Final loss is: 13.879608154296875\n",
      "Epoch: 166 Final loss is: 27.259384155273438\n",
      "Epoch: 167 Final loss is: 80.89015197753906\n",
      "Epoch: 168 Final loss is: 243.1759490966797\n",
      "Epoch: 169 Final loss is: 132.1002960205078\n",
      "Epoch: 170 Final loss is: 41.58998107910156\n",
      "Epoch: 171 Final loss is: 120.01467895507812\n",
      "Epoch: 172 Final loss is: 137.92324829101562\n",
      "Epoch: 173 Final loss is: 138.11965942382812\n",
      "Epoch: 174 Final loss is: 138.9823760986328\n",
      "Epoch: 175 Final loss is: 83.57652282714844\n",
      "Epoch: 176 Final loss is: 21.064453125\n",
      "Epoch: 177 Final loss is: 17.874984741210938\n",
      "Epoch: 178 Final loss is: 54.43357849121094\n",
      "Epoch: 179 Final loss is: 143.71644592285156\n",
      "Epoch: 180 Final loss is: 13.597671508789062\n",
      "Epoch: 181 Final loss is: 117.73179626464844\n",
      "Epoch: 182 Final loss is: 132.48922729492188\n",
      "Epoch: 183 Final loss is: 2.5047454833984375\n",
      "Epoch: 184 Final loss is: 11.112319946289062\n",
      "Epoch: 185 Final loss is: 588.734375\n",
      "Epoch: 186 Final loss is: 58.67877197265625\n",
      "Epoch: 187 Final loss is: 177.3811798095703\n",
      "Epoch: 188 Final loss is: 27.517364501953125\n",
      "Epoch: 189 Final loss is: 1.1162872314453125\n",
      "Epoch: 190 Final loss is: 53.71192932128906\n",
      "Epoch: 191 Final loss is: 293.85394287109375\n",
      "Epoch: 192 Final loss is: 3.834808349609375\n",
      "Epoch: 193 Final loss is: 130.1859130859375\n",
      "Epoch: 194 Final loss is: 215.01953125\n",
      "Epoch: 195 Final loss is: 67.85528564453125\n",
      "Epoch: 196 Final loss is: 5.1477203369140625\n",
      "Epoch: 197 Final loss is: 231.49522399902344\n",
      "Epoch: 198 Final loss is: 549.738525390625\n",
      "Epoch: 199 Final loss is: 72.53487396240234\n",
      "Epoch: 200 Final loss is: 146.36769104003906\n",
      "Epoch: 201 Final loss is: 12.792160034179688\n",
      "Epoch: 202 Final loss is: 170.80052185058594\n",
      "Epoch: 203 Final loss is: 547.8638305664062\n",
      "Epoch: 204 Final loss is: 332.78985595703125\n",
      "Epoch: 205 Final loss is: 11.648170471191406\n",
      "Epoch: 206 Final loss is: 101.62901306152344\n",
      "Epoch: 207 Final loss is: 168.92945861816406\n",
      "Epoch: 208 Final loss is: 0.449005126953125\n",
      "Epoch: 209 Final loss is: 34.20399475097656\n",
      "Epoch: 210 Final loss is: 43.28570556640625\n",
      "Epoch: 211 Final loss is: 65.77043151855469\n",
      "Epoch: 212 Final loss is: 16.450332641601562\n",
      "Epoch: 213 Final loss is: 113.80035400390625\n",
      "Epoch: 214 Final loss is: 166.07406616210938\n",
      "Epoch: 215 Final loss is: 465.4662780761719\n",
      "Epoch: 216 Final loss is: 2.8133544921875\n",
      "Epoch: 217 Final loss is: 107.60809326171875\n",
      "Epoch: 218 Final loss is: 51.9014892578125\n",
      "Epoch: 219 Final loss is: 152.27015686035156\n",
      "Epoch: 220 Final loss is: 283.2518310546875\n",
      "Epoch: 221 Final loss is: 849.7462158203125\n",
      "Epoch: 222 Final loss is: 0.4572725296020508\n",
      "Epoch: 223 Final loss is: 34.537994384765625\n",
      "Epoch: 224 Final loss is: 13.808685302734375\n",
      "Epoch: 225 Final loss is: 847.4072875976562\n",
      "Epoch: 226 Final loss is: 19.370147705078125\n",
      "Epoch: 227 Final loss is: 54.25006103515625\n",
      "Epoch: 228 Final loss is: 130.762939453125\n",
      "Epoch: 229 Final loss is: 27.988311767578125\n",
      "Epoch: 230 Final loss is: 108.0372314453125\n",
      "Epoch: 231 Final loss is: 652.9661254882812\n",
      "Epoch: 232 Final loss is: 5.3519439697265625\n",
      "Epoch: 233 Final loss is: 83.03990173339844\n",
      "Epoch: 234 Final loss is: 10.012557983398438\n",
      "Epoch: 235 Final loss is: 53.52838134765625\n",
      "Epoch: 236 Final loss is: 177.8643798828125\n",
      "Epoch: 237 Final loss is: 116.08076477050781\n",
      "Epoch: 238 Final loss is: 100.60551452636719\n",
      "Epoch: 239 Final loss is: 2.124606132507324\n",
      "Epoch: 240 Final loss is: 1108.0098876953125\n",
      "Epoch: 241 Final loss is: 121.55436706542969\n",
      "Epoch: 242 Final loss is: 27.511367797851562\n",
      "Epoch: 243 Final loss is: 312.7894592285156\n",
      "Epoch: 244 Final loss is: 61.62153625488281\n",
      "Epoch: 245 Final loss is: 135.3603973388672\n",
      "Epoch: 246 Final loss is: 0.6891708374023438\n",
      "Epoch: 247 Final loss is: 6.2689361572265625\n",
      "Epoch: 248 Final loss is: 69.50669860839844\n",
      "Epoch: 249 Final loss is: 66.68434143066406\n",
      "Epoch: 250 Final loss is: 71.97933959960938\n",
      "Epoch: 251 Final loss is: 147.7049102783203\n",
      "Epoch: 252 Final loss is: 677.2366943359375\n",
      "Epoch: 253 Final loss is: 28.727035522460938\n",
      "Epoch: 254 Final loss is: 23.2960205078125\n",
      "Epoch: 255 Final loss is: 68.82452392578125\n",
      "Epoch: 256 Final loss is: 14.944778442382812\n",
      "Epoch: 257 Final loss is: 90.95498657226562\n",
      "Epoch: 258 Final loss is: 55.16925048828125\n",
      "Epoch: 259 Final loss is: 60.025482177734375\n",
      "Epoch: 260 Final loss is: 44.65626525878906\n",
      "Epoch: 261 Final loss is: 124.18075561523438\n",
      "Epoch: 262 Final loss is: 110.72572326660156\n",
      "Epoch: 263 Final loss is: 858.6996459960938\n",
      "Epoch: 264 Final loss is: 49.7611083984375\n",
      "Epoch: 265 Final loss is: 29.06781005859375\n",
      "Epoch: 266 Final loss is: 367.8782958984375\n",
      "Epoch: 267 Final loss is: 175.96481323242188\n",
      "Epoch: 268 Final loss is: 30.128860473632812\n",
      "Epoch: 269 Final loss is: 96.52146911621094\n",
      "Epoch: 270 Final loss is: 22.548431396484375\n",
      "Epoch: 271 Final loss is: 52.308685302734375\n",
      "Epoch: 272 Final loss is: 35.283233642578125\n",
      "Epoch: 273 Final loss is: 696.6359252929688\n",
      "Epoch: 274 Final loss is: 96.70626831054688\n",
      "Epoch: 275 Final loss is: 117.37138366699219\n",
      "Epoch: 276 Final loss is: 85.49943542480469\n",
      "Epoch: 277 Final loss is: 111.37857055664062\n",
      "Epoch: 278 Final loss is: 1778.976318359375\n",
      "Epoch: 279 Final loss is: 82.10781860351562\n",
      "Epoch: 280 Final loss is: 1.1045160293579102\n",
      "Epoch: 281 Final loss is: 114.91650390625\n",
      "Epoch: 282 Final loss is: 397.866455078125\n",
      "Epoch: 283 Final loss is: 78.40847778320312\n",
      "Epoch: 284 Final loss is: 144.9759979248047\n",
      "Epoch: 285 Final loss is: 83.54649353027344\n",
      "Epoch: 286 Final loss is: 163.92393493652344\n",
      "Epoch: 287 Final loss is: 236.99879455566406\n",
      "Epoch: 288 Final loss is: 105.53837585449219\n",
      "Epoch: 289 Final loss is: 1.0131726264953613\n",
      "Epoch: 290 Final loss is: 121.89712524414062\n",
      "Epoch: 291 Final loss is: 11.707977294921875\n",
      "Epoch: 292 Final loss is: 120.86274719238281\n",
      "Epoch: 293 Final loss is: 54.97344970703125\n",
      "Epoch: 294 Final loss is: 1837.380615234375\n",
      "Epoch: 295 Final loss is: 36.05595397949219\n",
      "Epoch: 296 Final loss is: 141.64352416992188\n",
      "Epoch: 297 Final loss is: 16.457473754882812\n",
      "Epoch: 298 Final loss is: 121.06080627441406\n",
      "Epoch: 299 Final loss is: 35.45643615722656\n",
      "Epoch: 300 Final loss is: 2.582916259765625\n",
      "Epoch: 301 Final loss is: 111.10971069335938\n",
      "Epoch: 302 Final loss is: 401.94012451171875\n",
      "Epoch: 303 Final loss is: 46.059234619140625\n",
      "Epoch: 304 Final loss is: 163.4556121826172\n",
      "Epoch: 305 Final loss is: 233.9216766357422\n",
      "Epoch: 306 Final loss is: 21.449081420898438\n",
      "Epoch: 307 Final loss is: 50.599945068359375\n",
      "Epoch: 308 Final loss is: 45.22679138183594\n",
      "Epoch: 309 Final loss is: 101.31356811523438\n",
      "Epoch: 310 Final loss is: 211.58001708984375\n",
      "Epoch: 311 Final loss is: 2103.162109375\n",
      "Epoch: 312 Final loss is: 23.275741577148438\n",
      "Epoch: 313 Final loss is: 26.482940673828125\n",
      "Epoch: 314 Final loss is: 3.88318133354187\n",
      "Epoch: 315 Final loss is: 266.09808349609375\n",
      "Epoch: 316 Final loss is: 14.883926391601562\n",
      "Epoch: 317 Final loss is: 71.48625183105469\n",
      "Epoch: 318 Final loss is: 42.63043212890625\n",
      "Epoch: 319 Final loss is: 21.336181640625\n",
      "Epoch: 320 Final loss is: 103.39579772949219\n",
      "Epoch: 321 Final loss is: 2104.93603515625\n",
      "Epoch: 322 Final loss is: 22.19940185546875\n",
      "Epoch: 323 Final loss is: 13.09600830078125\n",
      "Epoch: 324 Final loss is: 44.54229736328125\n",
      "Epoch: 325 Final loss is: 12.62164306640625\n",
      "Epoch: 326 Final loss is: 98.41702270507812\n",
      "Epoch: 327 Final loss is: 38.01837158203125\n",
      "Epoch: 328 Final loss is: 79.93193054199219\n",
      "Epoch: 329 Final loss is: 70.17958068847656\n",
      "Epoch: 330 Final loss is: 86.38481140136719\n",
      "Epoch: 331 Final loss is: 114.52969360351562\n",
      "Epoch: 332 Final loss is: 341.1062316894531\n",
      "Epoch: 333 Final loss is: 29.944686889648438\n",
      "Epoch: 334 Final loss is: 59.191925048828125\n",
      "Epoch: 335 Final loss is: 464.8100280761719\n",
      "Epoch: 336 Final loss is: 34.93898010253906\n",
      "Epoch: 337 Final loss is: 532.702392578125\n",
      "Epoch: 338 Final loss is: 5.5597686767578125\n",
      "Epoch: 339 Final loss is: 108.548828125\n",
      "Epoch: 340 Final loss is: 122.80525207519531\n",
      "Epoch: 341 Final loss is: 49.18180847167969\n",
      "Epoch: 342 Final loss is: 43.754241943359375\n",
      "Epoch: 343 Final loss is: 156.49571228027344\n",
      "Epoch: 344 Final loss is: 264.26080322265625\n",
      "Epoch: 345 Final loss is: 1.953643798828125\n",
      "Epoch: 346 Final loss is: 97.05517578125\n",
      "Epoch: 347 Final loss is: 50.97393798828125\n",
      "Epoch: 348 Final loss is: 22.51580810546875\n",
      "Epoch: 349 Final loss is: 14.5084228515625\n",
      "Epoch: 350 Final loss is: 127.79251098632812\n",
      "Epoch: 351 Final loss is: 39.167236328125\n",
      "Epoch: 352 Final loss is: 140.34815979003906\n",
      "Epoch: 353 Final loss is: 105.86123657226562\n",
      "Epoch: 354 Final loss is: 93.64427185058594\n",
      "Epoch: 355 Final loss is: 71.080322265625\n",
      "Epoch: 356 Final loss is: 944.1488037109375\n",
      "Epoch: 357 Final loss is: 138.04661560058594\n",
      "Epoch: 358 Final loss is: 75.87156677246094\n",
      "Epoch: 359 Final loss is: 27.783706665039062\n",
      "Epoch: 360 Final loss is: 65.3436279296875\n",
      "Epoch: 361 Final loss is: 26.553482055664062\n",
      "Epoch: 362 Final loss is: 133.92857360839844\n",
      "Epoch: 363 Final loss is: 101.10847473144531\n",
      "Epoch: 364 Final loss is: 217.721923828125\n",
      "Epoch: 365 Final loss is: 46.103363037109375\n",
      "Epoch: 366 Final loss is: 25.933929443359375\n",
      "Epoch: 367 Final loss is: 210.94935607910156\n",
      "Epoch: 368 Final loss is: 122.62835693359375\n",
      "Epoch: 369 Final loss is: 257.94622802734375\n",
      "Epoch: 370 Final loss is: 10.544998168945312\n",
      "Epoch: 371 Final loss is: 1004.7501220703125\n",
      "Epoch: 372 Final loss is: 261.13751220703125\n",
      "Epoch: 373 Final loss is: 6.9723968505859375\n",
      "Epoch: 374 Final loss is: 343.552490234375\n",
      "Epoch: 375 Final loss is: 241.74960327148438\n",
      "Epoch: 376 Final loss is: 169.1501922607422\n",
      "Epoch: 377 Final loss is: 55.10980224609375\n",
      "Epoch: 378 Final loss is: 206.79693603515625\n",
      "Epoch: 379 Final loss is: 72.25776672363281\n",
      "Epoch: 380 Final loss is: 45.310302734375\n",
      "Epoch: 381 Final loss is: 30.138275146484375\n",
      "Epoch: 382 Final loss is: 138.3617706298828\n",
      "Epoch: 383 Final loss is: 70.38856506347656\n",
      "Epoch: 384 Final loss is: 217.25033569335938\n",
      "Epoch: 385 Final loss is: 652.1829833984375\n",
      "Epoch: 386 Final loss is: 654.4367065429688\n",
      "Epoch: 387 Final loss is: 157.9984893798828\n",
      "Epoch: 388 Final loss is: 93.33735656738281\n",
      "Epoch: 389 Final loss is: 91.56375122070312\n",
      "Epoch: 390 Final loss is: 125.61888122558594\n",
      "Epoch: 391 Final loss is: 37.43055725097656\n",
      "Epoch: 392 Final loss is: 3.6004300117492676\n",
      "Epoch: 393 Final loss is: 147.048828125\n",
      "Epoch: 394 Final loss is: 3.190873146057129\n",
      "Epoch: 395 Final loss is: 170.28805541992188\n",
      "Epoch: 396 Final loss is: 1779.0975341796875\n",
      "Epoch: 397 Final loss is: 21.4676513671875\n",
      "Epoch: 398 Final loss is: 98.6390380859375\n",
      "Epoch: 399 Final loss is: 87.5316162109375\n",
      "Epoch: 400 Final loss is: 3.1574153900146484\n",
      "Epoch: 401 Final loss is: 55.6026611328125\n",
      "Epoch: 402 Final loss is: 13.568145751953125\n",
      "Epoch: 403 Final loss is: 121.06692504882812\n",
      "Epoch: 404 Final loss is: 1057.0245361328125\n",
      "Epoch: 405 Final loss is: 1.3367433547973633\n",
      "Epoch: 406 Final loss is: 62.46209716796875\n",
      "Epoch: 407 Final loss is: 296.9638366699219\n",
      "Epoch: 408 Final loss is: 26.58587646484375\n",
      "Epoch: 409 Final loss is: 5.1741180419921875\n",
      "Epoch: 410 Final loss is: 24.004608154296875\n",
      "Epoch: 411 Final loss is: 898.9341430664062\n",
      "Epoch: 412 Final loss is: 517.486328125\n",
      "Epoch: 413 Final loss is: 12.443572998046875\n",
      "Epoch: 414 Final loss is: 95.34197998046875\n",
      "Epoch: 415 Final loss is: 69.99722290039062\n",
      "Epoch: 416 Final loss is: 250.90206909179688\n",
      "Epoch: 417 Final loss is: 0.36307334899902344\n",
      "Epoch: 418 Final loss is: 76.36965942382812\n",
      "Epoch: 419 Final loss is: 144.0855712890625\n",
      "Epoch: 420 Final loss is: 210.25082397460938\n",
      "Epoch: 421 Final loss is: 121.42404174804688\n",
      "Epoch: 422 Final loss is: 1557.5919189453125\n",
      "Epoch: 423 Final loss is: 44.802520751953125\n",
      "Epoch: 424 Final loss is: 108.42510986328125\n",
      "Epoch: 425 Final loss is: 80.15690612792969\n",
      "Epoch: 426 Final loss is: 96.66806030273438\n",
      "Epoch: 427 Final loss is: 311.71337890625\n",
      "Epoch: 428 Final loss is: 100.23504638671875\n",
      "Epoch: 429 Final loss is: 673.8663940429688\n",
      "Epoch: 430 Final loss is: 117.6004638671875\n",
      "Epoch: 431 Final loss is: 272.25677490234375\n",
      "Epoch: 432 Final loss is: 46.12919616699219\n",
      "Epoch: 433 Final loss is: 332.5404052734375\n",
      "Epoch: 434 Final loss is: 373.2814636230469\n",
      "Epoch: 435 Final loss is: 393.6370849609375\n",
      "Epoch: 436 Final loss is: 16.88763427734375\n",
      "Epoch: 437 Final loss is: 88.9014892578125\n",
      "Epoch: 438 Final loss is: 6.2581634521484375\n",
      "Epoch: 439 Final loss is: 40.772216796875\n",
      "Epoch: 440 Final loss is: 16.89434814453125\n",
      "Epoch: 441 Final loss is: 62.10462951660156\n",
      "Epoch: 442 Final loss is: 240.1380157470703\n",
      "Epoch: 443 Final loss is: 45.60804748535156\n",
      "Epoch: 444 Final loss is: 5.338348388671875\n",
      "Epoch: 445 Final loss is: 5.4005584716796875\n",
      "Epoch: 446 Final loss is: 51.65687561035156\n",
      "Epoch: 447 Final loss is: 145.88023376464844\n",
      "Epoch: 448 Final loss is: 120.700439453125\n",
      "Epoch: 449 Final loss is: 158.78103637695312\n",
      "Epoch: 450 Final loss is: 91.08224487304688\n",
      "Epoch: 451 Final loss is: 114.46664428710938\n",
      "Epoch: 452 Final loss is: 295.69146728515625\n",
      "Epoch: 453 Final loss is: 20.778488159179688\n",
      "Epoch: 454 Final loss is: 74.51493835449219\n",
      "Epoch: 455 Final loss is: 71.64601135253906\n",
      "Epoch: 456 Final loss is: 137.27310180664062\n",
      "Epoch: 457 Final loss is: 277.37017822265625\n",
      "Epoch: 458 Final loss is: 1.52044677734375\n",
      "Epoch: 459 Final loss is: 116.0242919921875\n",
      "Epoch: 460 Final loss is: 271.369873046875\n",
      "Epoch: 461 Final loss is: 71.56208801269531\n",
      "Epoch: 462 Final loss is: 229.0729522705078\n",
      "Epoch: 463 Final loss is: 15.677566528320312\n",
      "Epoch: 464 Final loss is: 701.4700317382812\n",
      "Epoch: 465 Final loss is: 778.349853515625\n",
      "Epoch: 466 Final loss is: 60.07905578613281\n",
      "Epoch: 467 Final loss is: 81.2470703125\n",
      "Epoch: 468 Final loss is: 48.50709533691406\n",
      "Epoch: 469 Final loss is: 37.24748229980469\n",
      "Epoch: 470 Final loss is: 125.11122131347656\n",
      "Epoch: 471 Final loss is: 140.97471618652344\n",
      "Epoch: 472 Final loss is: 42.9500732421875\n",
      "Epoch: 473 Final loss is: 63.87675476074219\n",
      "Epoch: 474 Final loss is: 72.79367065429688\n",
      "Epoch: 475 Final loss is: 122.20295715332031\n",
      "Epoch: 476 Final loss is: 156.5807342529297\n",
      "Epoch: 477 Final loss is: 84.03886413574219\n",
      "Epoch: 478 Final loss is: 44.68226623535156\n",
      "Epoch: 479 Final loss is: 100.13479614257812\n",
      "Epoch: 480 Final loss is: 73.531982421875\n",
      "Epoch: 481 Final loss is: 2.1516032218933105\n",
      "Epoch: 482 Final loss is: 9.378952026367188\n",
      "Epoch: 483 Final loss is: 16.7398681640625\n",
      "Epoch: 484 Final loss is: 322.9673767089844\n",
      "Epoch: 485 Final loss is: 11.743148803710938\n",
      "Epoch: 486 Final loss is: 4966.16943359375\n",
      "Epoch: 487 Final loss is: 1147.944091796875\n",
      "Epoch: 488 Final loss is: 294.33636474609375\n",
      "Epoch: 489 Final loss is: 1.06610107421875\n",
      "Epoch: 490 Final loss is: 24.41424560546875\n",
      "Epoch: 491 Final loss is: 182.1334686279297\n",
      "Epoch: 492 Final loss is: 24.29412841796875\n",
      "Epoch: 493 Final loss is: 19.320892333984375\n",
      "Epoch: 494 Final loss is: 65.28941345214844\n",
      "Epoch: 495 Final loss is: 1956.3690185546875\n",
      "Epoch: 496 Final loss is: 840.6005859375\n",
      "Epoch: 497 Final loss is: 652.8792114257812\n",
      "Epoch: 498 Final loss is: 283.39801025390625\n",
      "Epoch: 499 Final loss is: 42.79661560058594\n",
      "Epoch: 499 Final loss is: 42.79661560058594\n"
     ]
    }
   ],
   "source": [
    "model = MultilayerPerceptron(input_size, 150, output_size)\n",
    "model = nn.Sequential(nn.Linear(input_size, 5), nn.ELU(),\n",
    "                     nn.Linear(5, 9), nn.ELU(),\n",
    "                     nn.Linear(9, 9), nn.ELU(),\n",
    "                     nn.Linear(9, 5), nn.ELU(),\n",
    "                     nn.Linear(5, 1))\n",
    "epochs = 500\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-4, momentum=0.99)\n",
    "training(model, train_loader, nn.L1Loss(), optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "interpreted-hartford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(138.0162)\n",
      "loss: tensor(121.0035)\n",
      "loss: tensor(592.4202)\n",
      "loss: tensor(470.4689)\n",
      "loss: tensor(203.6801)\n",
      "loss: tensor(110.0096)\n",
      "loss: tensor(123.6990)\n",
      "loss: tensor(173.9393)\n",
      "loss: tensor(0.0999)\n",
      "loss: tensor(0.0999)\n",
      "loss: tensor(195.3358)\n",
      "loss: tensor(147.4194)\n",
      "loss: tensor(178.4049)\n",
      "loss: tensor(146.3742)\n",
      "loss: tensor(565.1467)\n",
      "loss: tensor(146.5614)\n",
      "loss: tensor(23.6305)\n",
      "loss: tensor(394.2853)\n",
      "loss: tensor(366.4063)\n",
      "loss: tensor(494.4037)\n",
      "loss: tensor(23.6305)\n",
      "loss: tensor(39.8992)\n",
      "loss: tensor(189.4347)\n",
      "loss: tensor(173.9393)\n",
      "loss: tensor(237.5204)\n",
      "loss: tensor(420.1152)\n",
      "loss: tensor(187.2990)\n",
      "loss: tensor(205.6336)\n",
      "loss: tensor(50.6057)\n",
      "loss: tensor(118.6045)\n",
      "loss: tensor(497.5378)\n",
      "loss: tensor(19.3401)\n",
      "loss: tensor(0.0999)\n",
      "loss: tensor(183.5716)\n",
      "loss: tensor(329.0051)\n",
      "loss: tensor(305.0688)\n",
      "loss: tensor(143.9107)\n",
      "loss: tensor(184.3481)\n",
      "loss: tensor(1122.0887)\n",
      "loss: tensor(117.6321)\n",
      "loss: tensor(1122.0887)\n",
      "loss: tensor(23.6305)\n",
      "loss: tensor(237.5204)\n",
      "loss: tensor(141.7822)\n",
      "loss: tensor(0.0999)\n",
      "loss: tensor(267.6275)\n",
      "loss: tensor(168.8837)\n",
      "loss: tensor(197.7076)\n",
      "loss: tensor(322.8842)\n",
      "loss: tensor(2163.1467)\n",
      "loss: tensor(206.3881)\n",
      "loss: tensor(331.4083)\n",
      "loss: tensor(23.6305)\n",
      "loss: tensor(138.0162)\n",
      "loss: tensor(2.1080)\n",
      "loss: tensor(203.6801)\n",
      "loss: tensor(166.5162)\n",
      "loss: tensor(118.6045)\n",
      "loss: tensor(329.0051)\n",
      "loss: tensor(274.4444)\n",
      "loss: tensor(117.6321)\n",
      "loss: tensor(373.6175)\n",
      "loss: tensor(0.0999)\n",
      "loss: tensor(117.4417)\n",
      "loss: tensor(62.0572)\n",
      "loss: tensor(99.5333)\n",
      "loss: tensor(146.3742)\n",
      "loss: tensor(123.6990)\n",
      "loss: tensor(1981.0139)\n",
      "loss: tensor(138.0162)\n",
      "loss: tensor(123.6990)\n",
      "loss: tensor(206.3881)\n",
      "loss: tensor(327.0999)\n",
      "loss: tensor(62.0572)\n",
      "loss: tensor(147.4194)\n",
      "loss: tensor(166.5162)\n",
      "loss: tensor(470.4689)\n",
      "loss: tensor(0.0999)\n",
      "loss: tensor(23.6305)\n",
      "loss: tensor(350.5977)\n",
      "loss: tensor(0.0999)\n",
      "loss: tensor(276.2597)\n",
      "loss: tensor(2.1493)\n",
      "loss: tensor(113.4045)\n",
      "loss: tensor(74.7547)\n",
      "loss: tensor(74.7547)\n",
      "loss: tensor(503.7548)\n",
      "loss: tensor(634.7891)\n",
      "loss: tensor(117.6321)\n",
      "loss: tensor(168.8837)\n",
      "loss: tensor(165.9220)\n",
      "loss: tensor(23.6305)\n",
      "loss: tensor(189.4347)\n",
      "loss: tensor(121.0035)\n",
      "loss: tensor(121.0035)\n",
      "loss: tensor(282.2724)\n",
      "loss: tensor(0.0999)\n",
      "loss: tensor(50.6057)\n",
      "loss: tensor(893.6965)\n",
      "loss: tensor(752.1468)\n",
      "loss: tensor(118.6045)\n",
      "loss: tensor(274.4444)\n",
      "loss: tensor(74.7547)\n",
      "loss: tensor(23.6305)\n",
      "loss: tensor(121.0035)\n",
      "loss: tensor(861.6175)\n",
      "loss: tensor(494.4037)\n",
      "loss: tensor(118.6045)\n",
      "loss: tensor(184.3481)\n",
      "loss: tensor(138.2838)\n",
      "loss: tensor(373.9650)\n",
      "loss: tensor(84.1283)\n",
      "loss: tensor(457.4233)\n",
      "loss: tensor(824.0150)\n",
      "loss: tensor(222.3122)\n",
      "loss: tensor(197.7076)\n",
      "loss: tensor(305.0688)\n",
      "loss: tensor(197.7076)\n",
      "loss: tensor(165.9220)\n",
      "loss: tensor(237.5204)\n",
      "loss: tensor(84.1283)\n",
      "loss: tensor(103.0911)\n",
      "loss: tensor(117.4417)\n",
      "loss: tensor(99.5333)\n",
      "loss: tensor(50.6057)\n",
      "loss: tensor(131.6119)\n",
      "loss: tensor(303.8150)\n",
      "loss: tensor(117.6321)\n",
      "loss: tensor(121.0035)\n",
      "loss: tensor(183.5716)\n",
      "loss: tensor(202.0297)\n",
      "loss: tensor(237.5204)\n",
      "loss: tensor(274.4444)\n",
      "loss: tensor(138.0162)\n",
      "loss: tensor(190.8648)\n",
      "loss: tensor(324.7822)\n",
      "loss: tensor(693.4078)\n",
      "loss: tensor(206.3881)\n",
      "loss: tensor(187.2990)\n",
      "loss: tensor(84.1283)\n",
      "loss: tensor(257.2846)\n",
      "loss: tensor(147.4194)\n",
      "loss: tensor(202.0297)\n",
      "loss: tensor(383.4404)\n",
      "loss: tensor(618.6618)\n",
      "loss: tensor(118.6045)\n",
      "loss: tensor(200.3882)\n",
      "loss: tensor(84.1283)\n",
      "loss: tensor(141.7822)\n",
      "loss: tensor(203.6801)\n",
      "loss: tensor(117.6321)\n",
      "loss: tensor(206.3881)\n",
      "loss: tensor(23.6305)\n",
      "loss: tensor(74.7547)\n",
      "loss: tensor(23.6305)\n",
      "loss: tensor(84.1283)\n",
      "loss: tensor(0.0999)\n",
      "loss: tensor(50.6057)\n",
      "loss: tensor(232.7070)\n",
      "loss: tensor(274.4444)\n",
      "loss: tensor(2825.0266)\n",
      "loss: tensor(666.4297)\n",
      "loss: tensor(214.5653)\n",
      "loss: tensor(524.9049)\n",
      "loss: tensor(1701.0267)\n",
      "loss: tensor(214.5653)\n",
      "loss: tensor(443.2525)\n",
      "loss: tensor(99.4225)\n",
      "loss: tensor(592.4202)\n",
      "loss: tensor(457.4233)\n",
      "loss: tensor(211.4404)\n",
      "loss: tensor(222.1751)\n",
      "loss: tensor(329.0051)\n",
      "loss: tensor(0.0999)\n",
      "loss: tensor(267.4143)\n",
      "loss: tensor(303.8150)\n",
      "loss: tensor(171.9089)\n",
      "loss: tensor(23.6305)\n",
      "loss: tensor(241.6291)\n",
      "loss: tensor(190.8648)\n",
      "loss: tensor(23.6305)\n",
      "loss: tensor(186.5577)\n",
      "loss: tensor(524.9049)\n",
      "loss: tensor(147.4194)\n",
      "loss: tensor(331.4083)\n",
      "loss: tensor(118.6045)\n",
      "loss: tensor(121.0035)\n",
      "loss: tensor(222.1751)\n",
      "loss: tensor(324.7822)\n",
      "loss: tensor(147.3501)\n",
      "loss: tensor(147.3501)\n",
      "loss: tensor(386.3358)\n",
      "loss: tensor(254.1109)\n",
      "loss: tensor(39.8992)\n",
      "loss: tensor(2.1493)\n",
      "loss: tensor(5182.6289)\n",
      "loss: tensor(84.1283)\n",
      "loss: tensor(724.7189)\n",
      "loss: tensor(99.4225)\n",
      "loss: tensor(70.7070)\n",
      "loss: tensor(110.0096)\n",
      "loss: tensor(146.3742)\n",
      "loss: tensor(165.9220)\n",
      "loss: tensor(276.2597)\n",
      "loss: tensor(23.6305)\n",
      "loss: tensor(146.3742)\n",
      "loss: tensor(103.0911)\n",
      "loss: tensor(991.7271)\n",
      "loss: tensor(141.7822)\n",
      "loss: tensor(23.6305)\n",
      "loss: tensor(0.0999)\n",
      "loss: tensor(173.9393)\n",
      "loss: tensor(23.6305)\n",
      "loss: tensor(206.3881)\n",
      "loss: tensor(214.5653)\n",
      "loss: tensor(23.6305)\n",
      "loss: tensor(121.0035)\n",
      "loss: tensor(99.5333)\n",
      "loss: tensor(62.0572)\n",
      "loss: tensor(147.3501)\n",
      "loss: tensor(292.8042)\n",
      "loss: tensor(5064.1465)\n",
      "loss: tensor(0.0999)\n",
      "loss: tensor(56.9650)\n",
      "loss: tensor(74.7547)\n",
      "loss: tensor(0.0999)\n",
      "loss: tensor(117.6321)\n",
      "loss: tensor(548.4050)\n",
      "Total Loss L1: tensor(66152.0547)\n"
     ]
    }
   ],
   "source": [
    "evaluation(model, test_loader, nn.L1Loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-surgeon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
